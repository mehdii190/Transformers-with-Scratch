[
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "Path",
        "importPath": "pathlib",
        "description": "pathlib",
        "isExtraImport": true,
        "detail": "pathlib",
        "documentation": {}
    },
    {
        "label": "torch",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch",
        "description": "torch",
        "detail": "torch",
        "documentation": {}
    },
    {
        "label": "torch.nn",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torch.nn",
        "description": "torch.nn",
        "detail": "torch.nn",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "random_split",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "DataLoader",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "Dataset",
        "importPath": "torch.utils.data",
        "description": "torch.utils.data",
        "isExtraImport": true,
        "detail": "torch.utils.data",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "SummaryWriter",
        "importPath": "torch.utils.tensorboard",
        "description": "torch.utils.tensorboard",
        "isExtraImport": true,
        "detail": "torch.utils.tensorboard",
        "documentation": {}
    },
    {
        "label": "load_dataset",
        "importPath": "datasets",
        "description": "datasets",
        "isExtraImport": true,
        "detail": "datasets",
        "documentation": {}
    },
    {
        "label": "Tokenizer",
        "importPath": "tokenizers",
        "description": "tokenizers",
        "isExtraImport": true,
        "detail": "tokenizers",
        "documentation": {}
    },
    {
        "label": "WordLevel",
        "importPath": "tokenizers.models",
        "description": "tokenizers.models",
        "isExtraImport": true,
        "detail": "tokenizers.models",
        "documentation": {}
    },
    {
        "label": "WordLevelTrainer",
        "importPath": "tokenizers.trainers",
        "description": "tokenizers.trainers",
        "isExtraImport": true,
        "detail": "tokenizers.trainers",
        "documentation": {}
    },
    {
        "label": "Whitespace",
        "importPath": "tokenizers.pre_tokenizers",
        "description": "tokenizers.pre_tokenizers",
        "isExtraImport": true,
        "detail": "tokenizers.pre_tokenizers",
        "documentation": {}
    },
    {
        "label": "tqdm",
        "importPath": "tqdm",
        "description": "tqdm",
        "isExtraImport": true,
        "detail": "tqdm",
        "documentation": {}
    },
    {
        "label": "warnings",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "warnings",
        "description": "warnings",
        "detail": "warnings",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "BilingualDataset",
        "importPath": "dataset",
        "description": "dataset",
        "isExtraImport": true,
        "detail": "dataset",
        "documentation": {}
    },
    {
        "label": "causal_mask",
        "importPath": "dataset",
        "description": "dataset",
        "isExtraImport": true,
        "detail": "dataset",
        "documentation": {}
    },
    {
        "label": "build_transformer",
        "importPath": "model",
        "description": "model",
        "isExtraImport": true,
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "get_config",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "get_weights_file_path",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "latest_weights_file_path",
        "importPath": "config",
        "description": "config",
        "isExtraImport": true,
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "torchmetrics",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "torchmetrics",
        "description": "torchmetrics",
        "detail": "torchmetrics",
        "documentation": {}
    },
    {
        "label": "get_config",
        "kind": 2,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "def get_config():\n    return {\n        \"batch_size\": 8,\n        \"num_epochs\": 20,\n        \"lr\": 10**-4,\n        \"seq_len\": 350,\n        \"d_model\": 512,\n        \"datasource\": 'opus_books',\n        \"lang_src\": \"en\",\n        \"lang_tgt\": \"it\",",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "get_weights_file_path",
        "kind": 2,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "def get_weights_file_path(config, epoch: str):\n    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n    model_filename = f\"{config['model_basename']}{epoch}.pt\"\n    return str(Path('.') / model_folder / model_filename)\n# Find the latest weights file in the weights folder\ndef latest_weights_file_path(config):\n    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n    model_filename = f\"{config['model_basename']}*\"\n    weights_files = list(Path(model_folder).glob(model_filename))\n    if len(weights_files) == 0:",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "latest_weights_file_path",
        "kind": 2,
        "importPath": "config",
        "description": "config",
        "peekOfCode": "def latest_weights_file_path(config):\n    model_folder = f\"{config['datasource']}_{config['model_folder']}\"\n    model_filename = f\"{config['model_basename']}*\"\n    weights_files = list(Path(model_folder).glob(model_filename))\n    if len(weights_files) == 0:\n        return None\n    weights_files.sort()\n    return str(weights_files[-1])",
        "detail": "config",
        "documentation": {}
    },
    {
        "label": "BilingualDataset",
        "kind": 6,
        "importPath": "dataset",
        "description": "dataset",
        "peekOfCode": "class BilingualDataset(Dataset):\n    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len):\n        super().__init__()\n        self.seq_len = seq_len\n        self.ds = ds\n        self.tokenizer_src = tokenizer_src\n        self.tokenizer_tgt = tokenizer_tgt\n        self.src_lang = src_lang\n        self.tgt_lang = tgt_lang\n        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)",
        "detail": "dataset",
        "documentation": {}
    },
    {
        "label": "causal_mask",
        "kind": 2,
        "importPath": "dataset",
        "description": "dataset",
        "peekOfCode": "def causal_mask(size):\n    mask = torch.triu(torch.ones((1, size, size)), diagonal=1).type(torch.int)\n    return mask == 0",
        "detail": "dataset",
        "documentation": {}
    },
    {
        "label": "LayerNormalization",
        "kind": 6,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "class LayerNormalization(nn.Module):\n    def __init__(self, features: int, eps:float=10**-6) -> None:\n        super().__init__()\n        self.eps = eps\n        self.alpha = nn.Parameter(torch.ones(features)) # alpha is a learnable parameter\n        self.bias = nn.Parameter(torch.zeros(features)) # bias is a learnable parameter\n    def forward(self, x):\n        # x: (batch, seq_len, hidden_size)\n         # Keep the dimension for broadcasting\n        mean = x.mean(dim = -1, keepdim = True) # (batch, seq_len, 1)",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "FeedForwardBlock",
        "kind": 6,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "class FeedForwardBlock(nn.Module):\n    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n        super().__init__()\n        self.linear_1 = nn.Linear(d_model, d_ff) # w1 and b1\n        self.dropout = nn.Dropout(dropout)\n        self.linear_2 = nn.Linear(d_ff, d_model) # w2 and b2\n    def forward(self, x):\n        # (batch, seq_len, d_model) --> (batch, seq_len, d_ff) --> (batch, seq_len, d_model)\n        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\nclass InputEmbeddings(nn.Module):",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "InputEmbeddings",
        "kind": 6,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "class InputEmbeddings(nn.Module):\n    def __init__(self, d_model: int, vocab_size: int) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.vocab_size = vocab_size\n        self.embedding = nn.Embedding(vocab_size, d_model)\n    def forward(self, x):\n        # (batch, seq_len) --> (batch, seq_len, d_model)\n        # Multiply by sqrt(d_model) to scale the embeddings according to the paper\n        return self.embedding(x) * math.sqrt(self.d_model)",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "PositionalEncoding",
        "kind": 6,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "class PositionalEncoding(nn.Module):\n    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n        super().__init__()\n        self.d_model = d_model\n        self.seq_len = seq_len\n        self.dropout = nn.Dropout(dropout)\n        # Create a matrix of shape (seq_len, d_model)\n        pe = torch.zeros(seq_len, d_model)\n        # Create a vector of shape (seq_len)\n        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) # (seq_len, 1)",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "ResidualConnection",
        "kind": 6,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "class ResidualConnection(nn.Module):\n        def __init__(self, features: int, dropout: float) -> None:\n            super().__init__()\n            self.dropout = nn.Dropout(dropout)\n            self.norm = LayerNormalization(features)\n        def forward(self, x, sublayer):\n            return x + self.dropout(sublayer(self.norm(x)))\nclass MultiHeadAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n        super().__init__()",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "MultiHeadAttentionBlock",
        "kind": 6,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "class MultiHeadAttentionBlock(nn.Module):\n    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n        super().__init__()\n        self.d_model = d_model # Embedding vector size\n        self.h = h # Number of heads\n        # Make sure d_model is divisible by h\n        assert d_model % h == 0, \"d_model is not divisible by h\"\n        self.d_k = d_model // h # Dimension of vector seen by each head\n        self.w_q = nn.Linear(d_model, d_model, bias=False) # Wq\n        self.w_k = nn.Linear(d_model, d_model, bias=False) # Wk",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "EncoderBlock",
        "kind": 6,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "class EncoderBlock(nn.Module):\n    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(2)])\n    def forward(self, x, src_mask):\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask))\n        x = self.residual_connections[1](x, self.feed_forward_block)\n        return x",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "Encoder",
        "kind": 6,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "class Encoder(nn.Module):\n    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization(features)\n    def forward(self, x, mask):\n        for layer in self.layers:\n            x = layer(x, mask)\n        return self.norm(x)\nclass DecoderBlock(nn.Module):",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "DecoderBlock",
        "kind": 6,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "class DecoderBlock(nn.Module):\n    def __init__(self, features: int, self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n        super().__init__()\n        self.self_attention_block = self_attention_block\n        self.cross_attention_block = cross_attention_block\n        self.feed_forward_block = feed_forward_block\n        self.residual_connections = nn.ModuleList([ResidualConnection(features, dropout) for _ in range(3)])\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "Decoder",
        "kind": 6,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "class Decoder(nn.Module):\n    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n        super().__init__()\n        self.layers = layers\n        self.norm = LayerNormalization(features)\n    def forward(self, x, encoder_output, src_mask, tgt_mask):\n        for layer in self.layers:\n            x = layer(x, encoder_output, src_mask, tgt_mask)\n        return self.norm(x)\nclass ProjectionLayer(nn.Module):",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "ProjectionLayer",
        "kind": 6,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "class ProjectionLayer(nn.Module):\n    def __init__(self, d_model, vocab_size) -> None:\n        super().__init__()\n        self.proj = nn.Linear(d_model, vocab_size)\n    def forward(self, x) -> None:\n        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n        return self.proj(x)\nclass Transformer(nn.Module):\n    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n        super().__init__()",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "Transformer",
        "kind": 6,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "class Transformer(nn.Module):\n    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.src_embed = src_embed\n        self.tgt_embed = tgt_embed\n        self.src_pos = src_pos\n        self.tgt_pos = tgt_pos\n        self.projection_layer = projection_layer",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "build_transformer",
        "kind": 2,
        "importPath": "model",
        "description": "model",
        "peekOfCode": "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int=512, N: int=6, h: int=8, dropout: float=0.1, d_ff: int=2048) -> Transformer:\n    # Create the embedding layers\n    src_embed = InputEmbeddings(d_model, src_vocab_size)\n    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size)\n    # Create the positional encoding layers\n    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n    # Create the encoder blocks\n    encoder_blocks = []\n    for _ in range(N):",
        "detail": "model",
        "documentation": {}
    },
    {
        "label": "greedy_decode",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n    # Precompute the encoder output and reuse it for every step\n    encoder_output = model.encode(source, source_mask)\n    # Initialize the decoder input with the sos token\n    decoder_input = torch.empty(1, 1).fill_(sos_idx).type_as(source).to(device)\n    while True:\n        if decoder_input.size(1) == max_len:\n            break",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "run_validation",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_step, writer, num_examples=2):\n    model.eval()\n    count = 0\n    source_texts = []\n    expected = []\n    predicted = []\n    try:\n        # get the console window width\n        with os.popen('stty size', 'r') as console:\n            _, console_width = console.read().split()",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "get_all_sentences",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def get_all_sentences(ds, lang):\n    for item in ds:\n        yield item[\"translation\"][lang]\ndef get_or_build_tokenizer(config, ds, lang):\n    tokenizer_path = Path(config[\"tokenizer_file\"].format(lang))\n    if not Path.exists(tokenizer_path):\n        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n        tokenizer.pre_tokenizer = Whitespace()\n        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "get_or_build_tokenizer",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def get_or_build_tokenizer(config, ds, lang):\n    tokenizer_path = Path(config[\"tokenizer_file\"].format(lang))\n    if not Path.exists(tokenizer_path):\n        tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n        tokenizer.pre_tokenizer = Whitespace()\n        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\", \"[SOS]\", \"[EOS]\"], min_frequency=2)\n        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer=trainer)\n        tokenizer.save(str(tokenizer_path))\n    else:\n        tokenizer = Tokenizer.from_file(str(tokenizer_path))",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "get_ds",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def get_ds(config):\n    ds_raw = load_dataset(f\"{config['datasource']}\", f\"{config['lang_src']}-{config['lang_tgt']}\", split='train')\n    tokenizer_src = get_or_build_tokenizer(config, ds_raw, config[\"lang_src\"])\n    tokenizer_tgt = get_or_build_tokenizer(config, ds_raw, config[\"lang_tgt\"])\n    train_ds_size = int(0.9 * len(ds_raw))\n    val_ds_size = len(ds_raw) - train_ds_size\n    train_ds_raw , val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size])\n    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n    # Find the maximum length of each sentence in the source and target sentence",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "get_model",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def get_model(config, vocab_src_len, vocab_tgt_len):\n    model = build_transformer(vocab_src_len, vocab_tgt_len, config[\"seq_len\"], config['seq_len'], d_model=config['d_model'])\n    return model\ndef train_model(config):\n    # Define the device\n    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n    print(\"Using device:\", device)\n    if (device == 'cuda'):\n        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")",
        "detail": "train",
        "documentation": {}
    },
    {
        "label": "train_model",
        "kind": 2,
        "importPath": "train",
        "description": "train",
        "peekOfCode": "def train_model(config):\n    # Define the device\n    device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.has_mps or torch.backends.mps.is_available() else \"cpu\"\n    print(\"Using device:\", device)\n    if (device == 'cuda'):\n        print(f\"Device name: {torch.cuda.get_device_name(device.index)}\")\n        print(f\"Device memory: {torch.cuda.get_device_properties(device.index).total_memory / 1024 ** 3} GB\")\n    elif (device == 'mps'):\n        print(f\"Device name: <mps>\")\n    else:",
        "detail": "train",
        "documentation": {}
    }
]